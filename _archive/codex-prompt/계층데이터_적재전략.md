# 계층 데이터 적재 전략 (DX Platform)

## 문서 목적
본 문서는 DX 프로젝트에서 계층 데이터(트리 구조)를 중앙 데이터베이스(PostgreSQL)로 안정적으로 적재하기 위한 의도, 모델링 선택, 변환(ETL) 흐름, 그리고 API/DB 측 설계를 한국어로 정리합니다.

---

## 전체 흐름 개요
- 생산(Producer): DXrevit 또는 DXnavis에서 객체와 속성, 계층 관계를 추출
- 중개(API): FastAPI 서버(`/api/v1/ingest`)가 유효성 검증 후 DB에 기록
- 저장(DB): PostgreSQL의 `metadata / objects / relationships` 테이블 구조에 버전 단위로 스냅샷 적재

핵심은 “노드(objects) + 엣지(relationships)”의 평면 구조로 계층성을 표현하고, `model_version`을 통해 변경 이력을 스냅샷 버전으로 관리하는 것입니다.

---

## 데이터 모델링 선택
- 노드: `objects`
  - 키: `(model_version, object_id)` 고유
  - 속성은 `properties JSONB`로 보관(유연성/성능 균형)
- 엣지: `relationships`
  - 계층은 부모→자식 방향의 엣지로 표현
  - `relation_type` 예시: `Contains`(부모가 자식을 포함), `HostedBy` 등
  - `is_directed = true` 기본
- 버전: `metadata.model_version`로 모든 데이터 스코프를 분리(불변 스냅샷)

이 구조는 분석(뷰/함수), 버전 비교, 4D 매핑 등 상위 요구사항(Phase 3/4 문서)과 바로 접합됩니다.

---

## 소스 포맷과 권장 인터페이스
- 현재 소스: CSV(계층 속성 플랫 레코드)
- 권장 인터페이스: FastAPI Ingest JSON(ExtractedData 형태)
  - Metadata + Objects + Relationships로 구성된 단일 페이로드
  - 서버 측에서 검증 후 배치 저장

CSV를 계속 써도 되지만, 적재 시점에는 플랫폼 스키마(오브젝트/관계/메타데이터)로 변환해 API로 전송하는 브리지를 둡니다.

---

## CSV → 플랫폼 스키마 변환 규칙
입력 CSV 컬럼(예시):
```
ObjectId,ParentId,Level,DisplayName,Category,PropertyName,PropertyValue
```

1) Objects 구성
- `ObjectId` 기준으로 그룹화하여 한 개의 객체 레코드로 집계
- 대표 필드 채움: `category`(첫 행 또는 우선순위 규칙), `family/type`는 미제공 시 `null`
- `element_id`는 미제공이면 `0` 또는 `null`
- `activity_id`는 있으면 매핑, 없으면 `null`
- `properties`는 `{ PropertyName: PropertyValue }` 딕셔너리로 집계(JSONB)
- `bounding_box`는 미제공이면 `null`

2) Relationships 구성
- `ParentId != Guid.Empty` 인 행은 부모→자식 엣지 후보
- `source_object_id = ParentId`, `target_object_id = ObjectId`
- `relation_type = 'Contains'`, `is_directed = true`
- 중복 엣지는 제거(집계 시 Distinct)

3) Metadata 구성
- `model_version`: `프로젝트명_UTC타임스탬프` 권장(예: `MyProj_20250101_120000`)
- `timestamp`: UTC
- `project_name`, `created_by`, `description`, `revit_file_path` 등 채움
- `total_object_count = Objects 개수`

결과: `ExtractedData = { Metadata, Objects[], Relationships[] }`

---

## 적재 경로(우선순위)
1) API 경유(권장)
- 엔드포인트: `POST /api/v1/ingest`
- 본문: ExtractedData JSON
- 서버 처리: Pydantic 기반 유효성 검사 → 트랜잭션 내 일괄 INSERT

2) 직접 DB(대안)
- 순서: `metadata` → `objects`(배치) → `relationships`(배치)
- 제약: `uq_objects_version_objectid` 준수, 스냅샷 불변(UPDATE/DELETE 금지 트리거)
- 대량 적재 시 `COPY`/`executemany` 고려

---

## DB 스키마 요약(Phase 3)
- `metadata(model_version PK, timestamp timestamptz, project_name, created_by, ... )`
- `objects(id bigserial PK, model_version FK, object_id, category, family, type, activity_id, properties JSONB, bounding_box JSONB, ... , UNIQUE(model_version, object_id))`
- `relationships(id bigserial PK, model_version FK, source_object_id, target_object_id, relation_type, is_directed boolean, ..., UNIQUE(version, source, target, type))`
- 인덱스: 버전/카테고리/JSONB GIN 등 분석·조회 최적화 포함

---

## 성능 및 규모 전략
- 변환: CSV를 청크(예: 5~50만 행 단위)로 읽어 그룹화/집계
- 전송: API는 배치 단위(Objects/Relationships 수 제한)로 분할 전송 가능
- 저장: DB는 `executemany` 또는 `COPY` 활용, 트랜잭션 경계 명확화
- 인덱스: 대규모 최초 적재 시 인덱스 생성/재구축 타이밍 고려

---

## 검증/품질 보증
- 고아 노드 탐지: `ParentId != NULL` 이면서 부모 미존재 → 소스 데이터/매핑 규칙 점검
- 루트 개수/최대 깊이 점검: 재귀 CTE로 트리 구조 검증
- 버전별 카테고리 분포/관계 수: `analytics_version_summary` 뷰로 확인
- 버전 비교: `fn_compare_versions(v1, v2)`로 ADD/DELETE/MODIFIED 검증

---

## 오류 처리/가용성
- API 응답 실패 시 재시도(지수 백오프), 부분 실패 로깅
- 중복 방지: `(model_version, object_id)` 위배 시 전체/부분 롤백 또는 충돌 보고
- 운영: 로그 적재, 백업/복구 스크립트(Phase 3), 최소 권한 롤(Role) 사용

---

## 예시
1) CSV 샘플
```csv
ObjectId,ParentId,Level,DisplayName,Category,PropertyName,PropertyValue
0f2e...,00000000-0000-0000-0000-000000000000,0,Building,Model,Name,Main Bldg
1a3b...,0f2e...,1,Level 1,Level,Elevation,0.0
7c9d...,1a3b...,2,Wall-01,Walls,Length,5000
7c9d...,1a3b...,2,Wall-01,Walls,Height,3000
```

2) ExtractedData(JSON) 개념 형태
```json
{
  "Metadata": {
    "model_version": "MyProj_20250101_120000",
    "timestamp": "2025-01-01T12:00:00Z",
    "project_name": "MyProj",
    "created_by": "user1",
    "description": "정기 스냅샷",
    "total_object_count": 3,
    "revit_file_path": "C:/.../MyProj.rvt"
  },
  "Objects": [
    {"model_version":"MyProj_...","object_id":"0f2e...","element_id":0,"category":"Model","family":null,"type":null,"activity_id":null,"properties":{"Name":"Main Bldg"}},
    {"model_version":"MyProj_...","object_id":"1a3b...","element_id":0,"category":"Level","properties":{"Elevation":0.0}},
    {"model_version":"MyProj_...","object_id":"7c9d...","element_id":0,"category":"Walls","properties":{"Length":5000,"Height":3000}}
  ],
  "Relationships": [
    {"model_version":"MyProj_...","source_object_id":"0f2e...","target_object_id":"1a3b...","relation_type":"Contains","is_directed":true},
    {"model_version":"MyProj_...","source_object_id":"1a3b...","target_object_id":"7c9d...","relation_type":"Contains","is_directed":true}
  ]
}
```

---

## 마이그레이션/전환 권고
- 단기: CSV를 브리지 스크립트로 변환 후 API로 적재(현행 데이터 즉시 반영)
- 중기: 생산 측에서 곧바로 ExtractedData 구조(JSON) 출력하도록 전환
- 장기: 대용량 파이프라인(대규모 Bulk, 큐 기반 비동기 처리) 도입 검토

---

## 결론
계층 데이터는 트리 자체를 직렬화해 저장하기보다, “노드 + 엣지”로 정규화하여 버전 단위로 축적하고, 분석/비교/4D 연계는 뷰·함수에서 수행하는 전략을 채택합니다. 현재 CSV는 브리지 변환을 통해 손쉽게 이 구조로 흡수할 수 있으며, 향후에는 API 스키마(ExtractedData)에 직접 맞춘 JSON 출력으로 단순화하는 것이 바람직합니다.

